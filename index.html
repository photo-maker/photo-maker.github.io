<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>PhotoMaker</title>
<link href="./files/style.css" rel="stylesheet">
<script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./files/jquery.js"></script>
</head>

<body>
<div class="content">
  <div class="logo" style="text-align: center;">
    <a href="index.html">
      <img src="./assets/logo.png">
    </a>
  </div>
  <h1><strong>PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</strong></h1>
  <p id="authors"><a href="https://paper99.github.io/">Zhen Li</a	><sup>1,2*</sup> <a href="https://github.com/ljzycmd">Mingdeng Cao</a><sup>2,3*</sup> <a href="https://xinntao.github.io/">Xintao Wang</a><sup>2&#x2709;</sup> <a href="https://scholar.google.com/citations?user=zJvrrusAAAAJ&hl=en/">Zhongang Qi</a><sup>2</sup> <a href="https://mmcheng.net/cmm/">Ming-Ming Cheng</a><sup>1&#x2709;</sup> <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en/">Ying Shan</a><sup>2</sup><br>
    <br>
  <span style="font-size: 18px"><sup>1</sup>Nankai University &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>ARC Lab, Tencent PCG &nbsp;&nbsp;&nbsp;&nbsp; <sup>3</sup>University of Tokyo
  </span></p>
  <div style="text-align: center;">
    <span style="font-size: 14px"><sup>*</sup> Interns in ARC Lab, Tencent PCG  &nbsp;&nbsp;&nbsp;&nbsp;  &#x2709; Corresponding Authors</span>
  </div>
  <br>
  <br>
  <img src="./assets/teaser.jpg" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center"><em>Craft Your Reality with PhotoMaker: Where Imagination Meets Realism</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://github.com/TencentARC/PhotoMaker" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/TencentARC/PhotoMaker" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="files/todo.txt" target="_blank">[Demo]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	    <!-- (<font color="#C70039">new!</font>) <a href="https://github.com/TencentARC/PhotoMaker" target="_blank">[Dataset]</a> &nbsp;&nbsp;&nbsp;&nbsp; -->
            <a href="files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts. 
    However, existing personalized generation methods cannot simultaneously satisfy the requirements of high efficiency, promising identity (ID) fidelity, and flexible text controllability.
    In this work, we introduce <strong>PhotoMaker</strong>, an efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information.
    Such an embedding, serving as a unified ID representation, can not only encapsulate the characteristics of the same input ID comprehensively, but also accommodate the characteristics of different IDs for subsequent integration.
    This paves the way for more intriguing and practically valuable applications.
    Besides, to drive the training of our PhotoMaker, we propose an ID-oriented data construction pipeline to assemble the training data.
    Under the nourishment of the dataset constructed through the proposed pipeline,
    our PhotoMaker demonstrates better ID preservation ability than test-time fine-tuning based methods, yet provides significant speed improvements, high-quality generation results, strong generalization capabilities, and a wide range of applications.
  </p>
</div>
<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./assets/background.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2>Approach</h2>
  <p> Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. "dog"), and returns a fine-tuned/"personalized'' text-to-image model that encodes a unique identifier that refers to the subject. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts.</p>
  <br>
  <img class="summary-img" src="./assets/framework.jpg" style="width:80%;"> <br>
  <p>We first obtain the text embedding and image embeddings from text encoder(s) and image encoder, respectively.
    Then, we extract the fused embedding by merging the corresponding class embedding (e.g., man and woman) and each image embedding.
    Next, we concatenate all fused embeddings along the length dimension to form the <strong>stacked ID embedding</strong>.
    Finally, we feed the stacked ID embedding to all cross-attention layers for adaptively merging the ID content in the diffusion model.
  Note that although we use images of the same ID with the masked background during training, we can directly input images of different IDs without background distortion to create a new ID during inference.</p>
  <br>
</div>
<div class="content">
  <h2>Recontextualization</h2>
  <p>Results for re-contextualization of a bag and vase subject instances. By finetuning a model using our method we are able to generate different images of the a subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. </p>
  <img class="summary-img" src="./assets/website_recontext.jpg" style="width:100%;">
  <!-- <img class="summary-img" src="./assets/results.png" style="width:100%;"> -->
</div>

<div class="content">
  <h2>Bringing a person in artwork/old photo into reality</h2>
  <p>Results for re-contextualization of a bag and vase subject instances. By finetuning a model using our method we are able to generate different images of the a subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. </p>
  <img class="summary-img" src="./assets/website_oldphoto.jpg" style="width:100%;">
  <!-- <img class="summary-img" src="./assets/results.png" style="width:100%;"> -->
</div>

<div class="content">
  <h2>Stylization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./assets/stylization.jpg" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>Changing Age or Gender</h2>
  <p>Results for re-contextualization of a bag and vase subject instances. By finetuning a model using our method we are able to generate different images of the a subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. </p>
  <img class="summary-img" src="./assets/change_age_gender.jpg" style="width:100%;">
  <!-- <img class="summary-img" src="./assets/results.png" style="width:100%;"> -->
</div>


<div class="content">
  <h2>Identity Mixing</h2>
  <p>Original artistic renditions of our subject dog in the style of famous painters. We remark that many of the generated poses were not seen in the training set, such as the Van Gogh and Warhol rendition. We also note that some renditions seem to have novel composition and faithfully imitate the style of the painter - even suggesting some sort of creativity (extrapolation given previous knowledge).</p>
  <br>
  <img class="summary-img" src="./assets/identity_mixing.jpg" style="width:100%;"> <br>
  <br>
  We can balabala...
  <img class="summary-img" src="./assets/change_mix_ratio_image.jpg" style="width:100%;"> <br>
  <img class="summary-img" src="./assets/change_mix_ratio_embed.jpg" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>Comparisons</h2>
  <p>We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.</p>
  <br>
  <img class="summary-img" src="./assets/comparison_recontext.jpg" style="width:100%;"> <br>
</div>
<!-- <div class="content">
  <h2>Societal Impact</h2>
  <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns.</p>
  <br>
</div> -->
<div class="content">
  <h2>BibTex</h2>
  <code> @article{li2023photomaker,<br>
  &nbsp;&nbsp;title={PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding},<br>
  &nbsp;&nbsp;author={Li, Zhen and Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Cheng, Ming-Ming and Shan, Ying},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2312.12242},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code> 
</div>
<div class="content" id="acknowledgements">
  <p>
    <!-- <strong>Acknowledgements</strong>: -->
    <!-- If you want an image removed from this page or have other requests, please contact us at <a href="mailto:zhenli1031@gmail.com">zhenli1031@gmail.com</a>. -->
    <!-- <br> -->
    Our project page is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div>
</body>
</html>
